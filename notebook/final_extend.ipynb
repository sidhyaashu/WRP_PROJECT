{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Advanced NLP Approach for Automated Knowledge Gap Detection in Student Responses\n",
    "-------------------------------------------------------------------------------\n",
    "This script demonstrates:\n",
    "    - Advanced data pre-processing (tokenization, stopword removal, normalization, lemmatization)\n",
    "    - Semantic similarity analysis using Sentence-BERT\n",
    "    - Knowledge gap detection by comparing key concepts\n",
    "    - Detailed feedback generation for open-ended responses\n",
    "    - Visualization of data at each step of the pipeline\n",
    "\n",
    "Requirements:\n",
    "    - Python 3.x\n",
    "    - spaCy (and the English model: en_core_web_sm)\n",
    "    - nltk\n",
    "    - sentence-transformers\n",
    "    - scikit-learn\n",
    "    - numpy\n",
    "    - matplotlib\n",
    "    - seaborn\n",
    "    - wordcloud\n",
    "    - networkx\n",
    "\n",
    "To install required packages, you can run:\n",
    "    pip install spacy nltk sentence-transformers scikit-learn numpy matplotlib seaborn wordcloud networkx\n",
    "    python -m spacy download en_core_web_sm\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Download nltk data (if not already downloaded)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load Sentence-BERT Model\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# --------------------------\n",
    "# Data Collection (Sample Data)\n",
    "# --------------------------\n",
    "# Example open-ended questions, reference answers, and student responses\n",
    "questions = [\n",
    "    \"Explain the process of photosynthesis.\",\n",
    "    \"Describe the significance of the water cycle in Earth's ecosystem.\",\n",
    "    \"Discuss the impact of the Industrial Revolution on modern society.\"\n",
    "]\n",
    "\n",
    "reference_answers = [\n",
    "    \"Photosynthesis is the process by which green plants and some other organisms use sunlight to synthesize foods from carbon dioxide and water. It involves the chlorophyll in leaves and generates oxygen as a byproduct.\",\n",
    "    \"The water cycle is crucial as it distributes fresh water across the globe, supporting life. It involves processes like evaporation, condensation, precipitation, and infiltration, maintaining ecological balance.\",\n",
    "    \"The Industrial Revolution marked a major turning point in history; it led to advancements in technology, manufacturing, and transportation, significantly influencing modern society's economic and social structures.\"\n",
    "]\n",
    "\n",
    "student_responses = [\n",
    "    \"Plants use sunlight to make food from carbon dioxide and water, releasing oxygen.\",\n",
    "    \"Water evaporates, forms clouds, and comes back as rain, which is important for life.\",\n",
    "    \"The Industrial Revolution changed how things were made and had effects on today.\"\n",
    "]\n",
    "\n",
    "# Assign identifiers\n",
    "student_ids = [f'Student {i+1}' for i in range(len(student_responses))]\n",
    "\n",
    "# --------------------------\n",
    "# Pre-Processing Function\n",
    "# --------------------------\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess text by lowercasing, removing non-alphabetic characters, \n",
    "    stopwords, and performing lemmatization.\n",
    "    \"\"\"\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Remove stopwords and lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in STOPWORDS]\n",
    "    # Return cleaned text\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Preprocess all texts\n",
    "preprocessed_responses = [preprocess_text(resp) for resp in student_responses]\n",
    "preprocessed_references = [preprocess_text(ans) for ans in reference_answers]\n",
    "preprocessed_questions = [preprocess_text(q) for q in questions]\n",
    "\n",
    "# --------------------------\n",
    "# Visualization: Word Frequencies\n",
    "# --------------------------\n",
    "def plot_word_frequencies(texts, title):\n",
    "    \"\"\"\n",
    "    Plot the word frequency distribution of the provided texts.\n",
    "    \"\"\"\n",
    "    all_words = \" \".join(texts).split()\n",
    "    freq_dist = nltk.FreqDist(all_words)\n",
    "    freq_df = nltk.FreqDist(all_words).most_common(15)\n",
    "    words = [word for word, freq in freq_df]\n",
    "    freqs = [freq for word, freq in freq_df]\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.barplot(x=freqs, y=words, palette='viridis')\n",
    "    plt.title(f'Word Frequencies: {title}')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Words')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize word frequencies in student responses\n",
    "plot_word_frequencies(preprocessed_responses, \"Student Responses\")\n",
    "\n",
    "# Visualize word frequencies in reference answers\n",
    "plot_word_frequencies(preprocessed_references, \"Reference Answers\")\n",
    "\n",
    "# --------------------------\n",
    "# Semantic Similarity Analysis\n",
    "# --------------------------\n",
    "# Encode responses and reference answers\n",
    "response_embeddings = sbert_model.encode(preprocessed_responses)\n",
    "reference_embeddings = sbert_model.encode(preprocessed_references)\n",
    "\n",
    "# Compute similarity scores\n",
    "similarity_scores = []\n",
    "for resp_emb, ref_emb in zip(response_embeddings, reference_embeddings):\n",
    "    sim_score = util.cos_sim(resp_emb, ref_emb).item()\n",
    "    similarity_scores.append(sim_score)\n",
    "\n",
    "# Display similarity scores\n",
    "print(\"\\nSemantic Similarity Scores:\")\n",
    "for idx, score in enumerate(similarity_scores):\n",
    "    print(f\"Response {idx+1} Similarity: {score:.4f}\")\n",
    "\n",
    "# --------------------------\n",
    "# Visualization: Similarity Heatmap\n",
    "# --------------------------\n",
    "def plot_similarity_heatmap(response_embeddings, reference_embeddings):\n",
    "    \"\"\"\n",
    "    Plot a heatmap of similarity scores between all student responses and reference answers.\n",
    "    \"\"\"\n",
    "    sim_matrix = cosine_similarity(response_embeddings, reference_embeddings)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(sim_matrix, annot=True, cmap='coolwarm',\n",
    "                xticklabels=[f'Answer {i+1}' for i in range(len(reference_answers))],\n",
    "                yticklabels=student_ids, fmt=\".2f\")\n",
    "    plt.title('Semantic Similarity Heatmap')\n",
    "    plt.xlabel('Reference Answers')\n",
    "    plt.ylabel('Student Responses')\n",
    "    plt.show()\n",
    "\n",
    "plot_similarity_heatmap(response_embeddings, reference_embeddings)\n",
    "\n",
    "# --------------------------\n",
    "# Knowledge Gap Detection\n",
    "# --------------------------\n",
    "def extract_key_concepts(text):\n",
    "    \"\"\"\n",
    "    Extract key concepts (nouns and noun phrases) from the text.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    concepts = set()\n",
    "    for chunk in doc.noun_chunks:\n",
    "        concepts.add(chunk.lemma_.lower())\n",
    "    for token in doc:\n",
    "        if token.pos_ in ['NOUN', 'PROPN']:\n",
    "            concepts.add(token.lemma_.lower())\n",
    "    return concepts\n",
    "\n",
    "# Identify missing concepts in student responses\n",
    "def identify_missing_concepts(response_text, reference_text):\n",
    "    response_concepts = extract_key_concepts(response_text)\n",
    "    reference_concepts = extract_key_concepts(reference_text)\n",
    "    missing_concepts = reference_concepts - response_concepts\n",
    "    extra_concepts = response_concepts - reference_concepts\n",
    "    return missing_concepts, extra_concepts\n",
    "\n",
    "# --------------------------\n",
    "# Visualization: Concept Venn Diagram\n",
    "# --------------------------\n",
    "def plot_concept_venn_diagram(response_idx):\n",
    "    \"\"\"\n",
    "    Plot a Venn diagram of concepts in the student's response and reference answer.\n",
    "    \"\"\"\n",
    "    from matplotlib_venn import venn2\n",
    "\n",
    "    response_concepts = extract_key_concepts(preprocessed_responses[response_idx])\n",
    "    reference_concepts = extract_key_concepts(preprocessed_references[response_idx])\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    venn2([response_concepts, reference_concepts], set_labels=('Student Response', 'Reference Answer'))\n",
    "    plt.title(f'Concept Overlap for Response {response_idx+1}')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize concept overlap for each response\n",
    "for idx in range(len(student_responses)):\n",
    "    plot_concept_venn_diagram(idx)\n",
    "\n",
    "# --------------------------\n",
    "# Detailed Feedback Generation\n",
    "# --------------------------\n",
    "def generate_feedback(response_idx, threshold=0.75):\n",
    "    \"\"\"\n",
    "    Generate detailed feedback for a student's response.\n",
    "    \"\"\"\n",
    "    sim_score = similarity_scores[response_idx]\n",
    "    student_text = student_responses[response_idx]\n",
    "    reference_text = reference_answers[response_idx]\n",
    "    missing_concepts, extra_concepts = identify_missing_concepts(preprocessed_responses[response_idx], preprocessed_references[response_idx])\n",
    "\n",
    "    feedback = f\"Your Response:\\n{student_text}\\n\"\n",
    "    feedback += f\"\\nSimilarity Score: {sim_score:.4f}\\n\"\n",
    "    if sim_score >= threshold:\n",
    "        feedback += \"Great work! Your response covers the key concepts.\\n\"\n",
    "    else:\n",
    "        if missing_concepts:\n",
    "            feedback += \"\\nKey concepts you missed:\\n\"\n",
    "            feedback += \", \".join(missing_concepts) + \"\\n\"\n",
    "        if extra_concepts:\n",
    "            feedback += \"\\nAdditional concepts in your response:\\n\"\n",
    "            feedback += \", \".join(extra_concepts) + \"\\n\"\n",
    "        if not missing_concepts and not extra_concepts:\n",
    "            feedback += \"Your response could be elaborated further to include more details.\\n\"\n",
    "    return feedback\n",
    "\n",
    "# Generate feedback for each student response\n",
    "print(\"\\nDetailed Feedback:\")\n",
    "for idx in range(len(student_responses)):\n",
    "    print(f\"\\nFeedback for {student_ids[idx]}:\\n{generate_feedback(idx)}\")\n",
    "\n",
    "# --------------------------\n",
    "# Visualization: Missing Concepts Word Cloud\n",
    "# --------------------------\n",
    "def visualize_missing_concepts(response_idx):\n",
    "    \"\"\"\n",
    "    Create a word cloud of missing concepts for a student's response.\n",
    "    \"\"\"\n",
    "    missing_concepts, _ = identify_missing_concepts(preprocessed_responses[response_idx], preprocessed_references[response_idx])\n",
    "    if missing_concepts:\n",
    "        text = \" \".join(missing_concepts)\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='Reds').generate(text)\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Missing Concepts in {student_ids[response_idx]}')\n",
    "        plt.show()\n",
    "\n",
    "# Visualize missing concepts for each response\n",
    "for idx in range(len(student_responses)):\n",
    "    visualize_missing_concepts(idx)\n",
    "\n",
    "# --------------------------\n",
    "# Visualization: Similarity Score Distribution\n",
    "# --------------------------\n",
    "def plot_similarity_scores(similarity_scores):\n",
    "    \"\"\"\n",
    "    Plot similarity scores for student responses.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.barplot(x=student_ids, y=similarity_scores, palette='viridis')\n",
    "    plt.axhline(y=0.75, color='red', linestyle='--', label='Threshold')\n",
    "    plt.xlabel('Student Responses')\n",
    "    plt.ylabel('Similarity Score')\n",
    "    plt.title('Semantic Similarity Scores of Student Responses')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_similarity_scores(similarity_scores)\n",
    "\n",
    "# --------------------------\n",
    "# Visualization: Concept Network Graph\n",
    "# --------------------------\n",
    "def plot_concept_network(response_idx):\n",
    "    \"\"\"\n",
    "    Plot a network graph of concepts in the reference answer and student's response.\n",
    "    \"\"\"\n",
    "    missing_concepts, _ = identify_missing_concepts(preprocessed_responses[response_idx], preprocessed_references[response_idx])\n",
    "\n",
    "    ref_concepts = extract_key_concepts(preprocessed_references[response_idx])\n",
    "    doc = nlp(preprocessed_references[response_idx])\n",
    "\n",
    "    edges = []\n",
    "    for token in doc:\n",
    "        if token.text in ref_concepts:\n",
    "            for child in token.children:\n",
    "                if child.text in ref_concepts:\n",
    "                    edges.append((token.text, child.text))\n",
    "\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from(edges)\n",
    "\n",
    "    pos = nx.spring_layout(G, k=0.5, iterations=20)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    node_colors = []\n",
    "    for node in G.nodes():\n",
    "        if node in missing_concepts:\n",
    "            node_colors.append('red')\n",
    "        else:\n",
    "            node_colors.append('green')\n",
    "    \n",
    "    nx.draw(G, pos, with_labels=True, node_size=5000, node_color=node_colors, font_size=10)\n",
    "    plt.title(f'Concept Network for Reference Answer {response_idx+1}')\n",
    "    plt.show()\n",
    "\n",
    "# Plotting concept network for each reference answer\n",
    "for idx in range(len(reference_answers)):\n",
    "    plot_concept_network(idx)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
