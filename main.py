#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
A Multi-Layered NLP Approach for Automated Knowledge Gap Detection in Student Responses
------------------------------------------------------------------------------------------
This script demonstrates:
    - Data pre-processing (tokenization, stopword removal, normalization)
    - Dependency parsing (using spaCy)
    - Semantic similarity analysis (using Sentence-BERT)
    - Topic modeling (using Gensim LDA)
    
Requirements:
    - Python 3.x
    - spaCy (and the English model: en_core_web_sm)
    - nltk
    - sentence-transformers
    - gensim
    - numpy

To install required packages, you can run:
    pip install spacy nltk sentence-transformers gensim numpy
    python -m spacy download en_core_web_sm
"""

import re
import spacy
import nltk
from nltk.corpus import stopwords
from sentence_transformers import SentenceTransformer, util
import gensim
import gensim.corpora as corpora
import numpy as np

# Download nltk stopwords (if not already downloaded)
nltk.download('stopwords')
STOPWORDS = set(stopwords.words("english"))

# Load spaCy English model
nlp = spacy.load("en_core_web_sm")

# Load Sentence Transformer Model
sbert_model = SentenceTransformer('all-MiniLM-L6-v2')

# --------------------------
# Data Collection (Sample Data)
# --------------------------
# Example student responses and a reference answer
student_responses = [
    "The process of photosynthesis converts light energy into chemical energy.",
    "Photosynthesis is the method by which plants use sunlight to create food, but they need water too.",
    "Plants perform a process to turn sunlight into energy, though the details are not clear.",
    "Plants do something with light energy to make food."
]

reference_answer = "Photosynthesis is the process by which green plants convert light energy into chemical energy by synthesizing sugars from carbon dioxide and water."

# --------------------------
# Pre-Processing Function
# --------------------------
def preprocess_text(text):
    """
    Preprocess text by lowercasing, removing non-alphabetic characters, and stopwords.
    """
    # Lowercase the text
    text = text.lower()
    # Remove non-alphabetic characters (optional: you can adjust this as needed)
    text = re.sub(r'[^a-z\s]', '', text)
    # Tokenize and remove stopwords
    tokens = [word for word in text.split() if word not in STOPWORDS]
    # Return cleaned text
    return " ".join(tokens)

# Preprocess all responses
preprocessed_responses = [preprocess_text(resp) for resp in student_responses]
preprocessed_reference = preprocess_text(reference_answer)

# --------------------------
# Dependency Parsing Function
# --------------------------
def dependency_parse(text):
    """
    Parse the text and return its dependency tree information.
    """
    doc = nlp(text)
    dependencies = []
    for token in doc:
        dependencies.append((token.text, token.dep_, token.head.text))
    return dependencies

# Example: Print dependency parsing of the reference answer
print("Dependency Parsing (Reference Answer):")
for token, dep, head in dependency_parse(reference_answer):
    print(f"{token:12s} --> {dep:10s} --> {head}")

# --------------------------
# Semantic Similarity Analysis Function
# --------------------------
def compute_similarity(student_text, reference_text):
    """
    Compute cosine similarity between student text and reference text using Sentence-BERT embeddings.
    """
    # Compute embeddings
    embeddings = sbert_model.encode([student_text, reference_text], convert_to_tensor=True)
    # Compute cosine similarity (returns a tensor; convert to float)
    cosine_sim = util.cos_sim(embeddings[0], embeddings[1]).item()
    return cosine_sim

# Compute similarity scores for each student response
print("\nSemantic Similarity Scores:")
for idx, response in enumerate(preprocessed_responses):
    sim_score = compute_similarity(response, preprocessed_reference)
    print(f"Response {idx+1} Similarity: {sim_score:.4f}")

# --------------------------
# Topic Modeling with LDA Function
# --------------------------
def perform_topic_modeling(texts, num_topics=2, num_words=5):
    """
    Perform topic modeling using Gensim LDA on the provided texts.
    """
    # Tokenize texts for LDA (simple split, you can use more advanced tokenization if needed)
    tokenized_texts = [text.split() for text in texts]
    
    # Create Dictionary and Corpus
    dictionary = corpora.Dictionary(tokenized_texts)
    corpus = [dictionary.doc2bow(text) for text in tokenized_texts]
    
    # Build LDA model
    lda_model = gensim.models.LdaModel(corpus=corpus,
                                       id2word=dictionary,
                                       num_topics=num_topics,
                                       random_state=42,
                                       passes=10,
                                       alpha='auto',
                                       per_word_topics=True)
    
    topics = lda_model.print_topics(num_topics=num_topics, num_words=num_words)
    return topics

# Combine student responses and reference answer for topic modeling analysis
all_texts = preprocessed_responses + [preprocessed_reference]
topics = perform_topic_modeling(all_texts, num_topics=3, num_words=5)

print("\nIdentified Topics:")
for topic in topics:
    print(topic)

# --------------------------
# Evaluation Metrics (Sample Calculation)
# --------------------------
# In a full system, these metrics would be computed based on ground truth labels.
# Here, we simulate evaluation metrics with dummy values.

def calculate_evaluation_metrics(true_gaps, detected_gaps):
    """
    Simulate the calculation of precision, recall, and F1-score.
    true_gaps: set of actual knowledge gap identifiers (for demo, using dummy sets)
    detected_gaps: set of detected knowledge gap identifiers
    """
    true_positives = len(true_gaps.intersection(detected_gaps))
    precision = true_positives / len(detected_gaps) if detected_gaps else 0
    recall = true_positives / len(true_gaps) if true_gaps else 0
    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) else 0
    return precision, recall, f1_score

# Dummy ground truth and detected gaps (for demonstration)
true_gaps = {"concept_photosynthesis", "process_conversion"}
detected_gaps = {"concept_photosynthesis", "missing_water", "process_conversion"}

precision, recall, f1 = calculate_evaluation_metrics(true_gaps, detected_gaps)
print("\nEvaluation Metrics (Simulated):")
print(f"Precision: {precision:.2f}")
print(f"Recall:    {recall:.2f}")
print(f"F1-Score:  {f1:.2f}")

# --------------------------
# Main Pipeline Execution
# --------------------------
def main():
    print("\n--- Running Full Pipeline ---\n")
    
    # 1. Pre-process Responses
    print("Preprocessing student responses...")
    processed_responses = [preprocess_text(resp) for resp in student_responses]
    
    # 2. Dependency Parsing (Example on first response)
    print("\nDependency Parsing for first student response:")
    dependencies = dependency_parse(student_responses[0])
    for token, dep, head in dependencies:
        print(f"{token:12s} --> {dep:10s} --> {head}")
    
    # 3. Semantic Similarity (Example on first response)
    print("\nSemantic Similarity (first response vs. reference):")
    sim_score = compute_similarity(processed_responses[0], preprocessed_reference)
    print(f"Cosine Similarity: {sim_score:.4f}")
    
    # 4. Topic Modeling on all responses and reference
    print("\nPerforming Topic Modeling...")
    topics = perform_topic_modeling(processed_responses + [preprocessed_reference], num_topics=3, num_words=5)
    for topic in topics:
        print(topic)
    
    # 5. Simulated Evaluation Metrics
    print("\nSimulated Evaluation Metrics:")
    precision, recall, f1 = calculate_evaluation_metrics(true_gaps, detected_gaps)
    print(f"Precision: {precision:.2f}")
    print(f"Recall:    {recall:.2f}")
    print(f"F1-Score:  {f1:.2f}")
    
if __name__ == "__main__":
    main()
